{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File for finding the metrics of each of the graphs at different distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "from pyproj import Transformer\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "import geopandas as gpd\n",
    "import graph_functions as graf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_classified_thumbnails(folder_directory, data_folder, shp_dir, \n",
    "                                   n_clusters = 10, fishnet_rows = 5, \n",
    "                                   fishnet_cols = 5, scale = 1, xmeans = False, error_log = False):\n",
    "    \n",
    "    # cleaning out any extra files in case ran previously\n",
    "    delete_file_path = f'{shp_dir}/'\n",
    "    delete_file_paths = os.listdir(delete_file_path)\n",
    "    for file in delete_file_paths:\n",
    "        if (re.search(r'[0-9]_gcs+', file)) or (re.search(r'[a-zA-Z]_gcs+', file)):\n",
    "            full_path = f'{delete_file_path}{file}'\n",
    "            print(full_path)\n",
    "            os.remove(full_path)\n",
    "            print(f'removed {file}')\n",
    "    \n",
    "    # gaining a list of all the files in the given folder\n",
    "    shp_files = [f for f in os.listdir(shp_dir) if f.endswith('.shp')]\n",
    "\n",
    "    count = 0\n",
    "    # Iterate directory\n",
    "    for path in os.listdir(shp_dir):\n",
    "        # check if current path is a file\n",
    "        if path.endswith('.shp'):\n",
    "            count += 1\n",
    "    print(f'Downloading data for {count} files')\n",
    "    \n",
    "    index_num = -1\n",
    "    for shp_file in shp_files:\n",
    "        # increase the index by one for file searching and for creation later\n",
    "        index_num = index_num + 1\n",
    "        file_name_split = shp_file.split(\".\")\n",
    "        \n",
    "        # create the path to the start shapefile (polygon area)\n",
    "        shp_path = os.path.join(shp_dir, shp_file)\n",
    "        # create a feature collection from the feature\n",
    "        feature_collection = geemap.shp_to_ee(shp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some function examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'density : {test_graph.density()}')\n",
    "print(f'average path length : {test_graph.average_path_length()}')\n",
    "print(f'diameter : {test_graph.diameter()}')\n",
    "print(f'transitivity : {test_graph.transitivity_undirected()}')\n",
    "print(f\"kleinberg's hub score : {max(test_graph.hub_score())}\") # will need to attach to lek utm's\n",
    "print(f'cutpoints : {test_graph.articulation_points()}') # will need to attach to lek utm's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_directory = 'E:/!!Research/!!!Data/graph_analysis/igraph_graphs/coalescence'\n",
    "years_ex_list = []\n",
    "type_list = []\n",
    "mean_clust_info = []\n",
    "num_clust_info = []\n",
    "types_list = ['average', 'dispersal', 'coalescence', 'round', 'trip', '48', 'max']\n",
    "adjacency_files = [f for f in os.listdir(given_directory) if f.endswith('.adjacency')]\n",
    "\n",
    "for adj_file in adjacency_files:\n",
    "    file_path = f'{given_directory}/{adj_file}'\n",
    "    #parts = adj_file.split('_')\n",
    "    path_parts = adj_file.split('.')\n",
    "    parts = []\n",
    "    for piece in path_parts:\n",
    "        pieces = piece.split('_')\n",
    "        parts.append(pieces)\n",
    "    #print(parts[0][0])\n",
    "\n",
    "    #Create the graph from the file\n",
    "    graph = ig.Graph.Read_Adjacency(file_path)\n",
    "    # Cluster stuff\n",
    "    clusters = graph.connected_components()\n",
    "    # number of clusters\n",
    "    num_clust_info.append(len(clusters))\n",
    "    # mean size of clusters\n",
    "    cluster_sizes = clusters.sizes()\n",
    "    print(cluster_sizes)\n",
    "    mean_cluster_size = sum(cluster_sizes) / len(cluster_sizes)\n",
    "    mean_clust_info.append(mean_cluster_size)\n",
    "    \"\"\"\n",
    "    years_ex_list.append(parts[0])\n",
    "    found_types = []\n",
    "    for part in parts:\n",
    "        if part in types_list:\n",
    "            found_types.append(part)\n",
    "            \n",
    "    if found_types:\n",
    "        type_list.append('_'.join(found_types))\"\"\"\n",
    "            \n",
    "\n",
    "#print(years_ex_list)\n",
    "#print(type_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_metrics_df(given_directory):\n",
    "    # graph_type - already have a list of keywords to look for\n",
    "    types_list = ['average', 'dispersal', 'disperse', 'coalescence', 'round', 'trip', '48', 'max']\n",
    "    graph_type = []\n",
    "    # to easily find the year\n",
    "    years = []\n",
    "    densitys = []\n",
    "    avg_path_lengths = []\n",
    "    diameters = []\n",
    "    transitivitys = []\n",
    "    num_clusters = []\n",
    "    mean_clusters = []\n",
    "    max_cluster_sizes = []\n",
    "\n",
    "    adjacency_files = [f for f in os.listdir(f'{given_directory}') if f.endswith('.adjacency')]\n",
    "\n",
    "    for adj_file in adjacency_files:\n",
    "        file_path = f'{given_directory}/{adj_file}'\n",
    "        path_parts = adj_file.split('.')\n",
    "        parts = []\n",
    "        for piece in path_parts:\n",
    "            pieces = piece.split('_')\n",
    "            parts.append(pieces)\n",
    "        #parts = path_parts.split('_')\n",
    "        years.append(parts[0][0])\n",
    "        found_types = []\n",
    "        for part in parts[0]:\n",
    "            if part in types_list:\n",
    "                found_types.append(part)\n",
    "                \n",
    "        if found_types:\n",
    "            graph_type.append('_'.join(found_types))\n",
    "        \n",
    "        #Create the graph from the file\n",
    "        graph = ig.Graph.Read_Adjacency(file_path)\n",
    "        # Density\n",
    "        densitys.append(graph.density())\n",
    "        # Average path length\n",
    "        avg_path_lengths.append(graph.average_path_length())\n",
    "        # Diameter\n",
    "        diameters.append(graph.diameter())\n",
    "        # Undirected transitivity\n",
    "        transitivitys.append(graph.transitivity_undirected())\n",
    "        #\n",
    "        # Cluster stuff\n",
    "        clusters = graph.connected_components()\n",
    "        # number of clusters\n",
    "        num_clusters.append(len(clusters))\n",
    "        # mean size of clusters\n",
    "        cluster_sizes = clusters.sizes()\n",
    "        mean_cluster_size = sum(cluster_sizes) / len(cluster_sizes)\n",
    "        mean_clusters.append(mean_cluster_size)\n",
    "        # Max size of a cluster\n",
    "        max_cluster_sizes.append(max(cluster_sizes))\n",
    "        \n",
    "\n",
    "    data = {'graph_type' : graph_type,\n",
    "                'year' : years,\n",
    "                'density' : densitys,\n",
    "                'avg_path_length' : avg_path_lengths,\n",
    "                'diameter' : diameters,\n",
    "                'transitivity' : transitivitys,\n",
    "                'clusters' : num_clusters,\n",
    "                'mean_cluster_size' : mean_clusters,\n",
    "                'max_cluster_size' : max_cluster_sizes\n",
    "                }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_dispersal_df = global_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/average_dispersal')\n",
    "round_trip_df = global_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/round_trip')\n",
    "thresh48_df = global_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/threshold_4_8')\n",
    "max_disperse_df = global_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/threshold_max_disperse')\n",
    "coalescence_df = global_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/coalescence')\n",
    "\n",
    "frames = [average_dispersal_df, round_trip_df, thresh48_df, max_disperse_df, coalescence_df]\n",
    "frame_concat = pd.concat(frames)\n",
    "frame_concat.to_csv(\"E:/!!Research/!!!Data/graph_analysis/igraph_metrics_data/lek_rolling_binary_global_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per node metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per cluster metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

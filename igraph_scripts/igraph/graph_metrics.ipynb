{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File for finding the metrics of each of the graphs at different distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "from pyproj import Transformer\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "import geopandas as gpd\n",
    "import graph_functions as graf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some function examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'density : {test_graph.density()}')\n",
    "print(f'average path length : {test_graph.average_path_length()}')\n",
    "print(f'diameter : {test_graph.diameter()}')\n",
    "print(f'transitivity : {test_graph.transitivity_undirected()}')\n",
    "print(f\"kleinberg's hub score : {max(test_graph.hub_score())}\") # will need to attach to lek utm's\n",
    "print(f'cutpoints : {test_graph.articulation_points()}') # will need to attach to lek utm's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_directory = 'E:/!!Research/!!!Data/graph_analysis/igraph_graphs/coalescence'\n",
    "years_ex_list = []\n",
    "type_list = []\n",
    "mean_clust_info = []\n",
    "num_clust_info = []\n",
    "types_list = ['average', 'dispersal', 'coalescence', 'round', 'trip', '48', 'max']\n",
    "adjacency_files = [f for f in os.listdir(given_directory) if f.endswith('.adjacency')]\n",
    "\n",
    "for adj_file in adjacency_files:\n",
    "    file_path = f'{given_directory}/{adj_file}'\n",
    "    #parts = adj_file.split('_')\n",
    "    path_parts = adj_file.split('.')\n",
    "    parts = []\n",
    "    for piece in path_parts:\n",
    "        pieces = piece.split('_')\n",
    "        parts.append(pieces)\n",
    "    #print(parts[0][0])\n",
    "\n",
    "    #Create the graph from the file\n",
    "    graph = ig.Graph.Read_Adjacency(file_path)\n",
    "    # Cluster stuff\n",
    "    clusters = graph.connected_components()\n",
    "    # number of clusters\n",
    "    num_clust_info.append(len(clusters))\n",
    "    # mean size of clusters\n",
    "    cluster_sizes = clusters.sizes()\n",
    "    print(cluster_sizes)\n",
    "    mean_cluster_size = sum(cluster_sizes) / len(cluster_sizes)\n",
    "    mean_clust_info.append(mean_cluster_size)\n",
    "    \"\"\"\n",
    "    years_ex_list.append(parts[0])\n",
    "    found_types = []\n",
    "    for part in parts:\n",
    "        if part in types_list:\n",
    "            found_types.append(part)\n",
    "            \n",
    "    if found_types:\n",
    "        type_list.append('_'.join(found_types))\"\"\"\n",
    "            \n",
    "\n",
    "#print(years_ex_list)\n",
    "#print(type_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_metrics_df(given_directory):\n",
    "    # graph_type - already have a list of keywords to look for\n",
    "    types_list = ['average', 'dispersal', 'disperse', 'coalescence', 'round', 'trip', '48', 'max']\n",
    "    graph_type = []\n",
    "    # to easily find the year\n",
    "    years = []\n",
    "    densitys = []\n",
    "    avg_path_lengths = []\n",
    "    diameters = []\n",
    "    transitivitys = []\n",
    "    num_clusters = []\n",
    "    mean_clusters = []\n",
    "    max_cluster_sizes = []\n",
    "\n",
    "    adjacency_files = [f for f in os.listdir(f'{given_directory}') if f.endswith('.adjacency')]\n",
    "\n",
    "    for adj_file in adjacency_files:\n",
    "        file_path = f'{given_directory}/{adj_file}'\n",
    "        path_parts = adj_file.split('.')\n",
    "        parts = []\n",
    "        for piece in path_parts:\n",
    "            pieces = piece.split('_')\n",
    "            parts.append(pieces)\n",
    "        #parts = path_parts.split('_')\n",
    "        years.append(parts[0][0])\n",
    "        found_types = []\n",
    "        for part in parts[0]:\n",
    "            if part in types_list:\n",
    "                found_types.append(part)\n",
    "                \n",
    "        if found_types:\n",
    "            graph_type.append('_'.join(found_types))\n",
    "        \n",
    "        #Create the graph from the file\n",
    "        graph = ig.Graph.Read_Adjacency(file_path)\n",
    "        # Density\n",
    "        densitys.append(graph.density())\n",
    "        # Average path length\n",
    "        avg_path_lengths.append(graph.average_path_length())\n",
    "        # Diameter\n",
    "        diameters.append(graph.diameter())\n",
    "        # Undirected transitivity\n",
    "        transitivitys.append(graph.transitivity_undirected())\n",
    "        #\n",
    "        # Cluster stuff\n",
    "        clusters = graph.connected_components()\n",
    "        # number of clusters\n",
    "        num_clusters.append(len(clusters))\n",
    "        # mean size of clusters\n",
    "        cluster_sizes = clusters.sizes()\n",
    "        mean_cluster_size = sum(cluster_sizes) / len(cluster_sizes)\n",
    "        mean_clusters.append(mean_cluster_size)\n",
    "        # Max size of a cluster\n",
    "        max_cluster_sizes.append(max(cluster_sizes))\n",
    "        \n",
    "\n",
    "    data = {'graph_type' : graph_type,\n",
    "                'year' : years,\n",
    "                'density' : densitys,\n",
    "                'avg_path_length' : avg_path_lengths,\n",
    "                'diameter' : diameters,\n",
    "                'transitivity' : transitivitys,\n",
    "                'clusters' : num_clusters,\n",
    "                'mean_cluster_size' : mean_clusters,\n",
    "                'max_cluster_size' : max_cluster_sizes\n",
    "                }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_dispersal_df = global_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/average_dispersal')\n",
    "round_trip_df = global_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/round_trip')\n",
    "thresh48_df = global_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/threshold_4_8')\n",
    "max_disperse_df = global_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/threshold_max_disperse')\n",
    "coalescence_df = global_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/coalescence')\n",
    "\n",
    "frames = [average_dispersal_df, round_trip_df, thresh48_df, max_disperse_df, coalescence_df]\n",
    "frame_concat = pd.concat(frames)\n",
    "frame_concat.to_csv(\"E:/!!Research/!!!Data/graph_analysis/igraph_metrics_data/lek_rolling_binary_global_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per node metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_metrics_df(given_directory, given_df):\n",
    "    # graph_type - already have a list of keywords to look for\n",
    "    types_list = ['average', 'dispersal', 'disperse', 'coalescence', 'round', 'trip', '48', 'max']\n",
    "    leks = [] # lek id\n",
    "    # lek coordinates\n",
    "    x_easting = []\n",
    "    y_northing = []\n",
    "    \n",
    "    graph_type = [] # graph type\n",
    "    years = [] # to easily find the year\n",
    "    hub_scores = []\n",
    "    centrality_scores = []\n",
    "    #articulation_point = []\n",
    "\n",
    "\n",
    "    adjacency_files = [f for f in os.listdir(f'{given_directory}') if f.endswith('.adjacency')]\n",
    "\n",
    "    for adj_file in adjacency_files:\n",
    "        file_path = f'{given_directory}/{adj_file}'\n",
    "        path_parts = adj_file.split('.')\n",
    "        parts = []\n",
    "        for piece in path_parts:\n",
    "            pieces = piece.split('_')\n",
    "            parts.append(pieces)\n",
    "\n",
    "        year = parts[0][0]\n",
    "        year_df = given_df[given_df['year'] == int(year)]\n",
    "\n",
    "        found_types = []\n",
    "        for part in parts[0]:\n",
    "            if part in types_list:\n",
    "                found_types.append(part)\n",
    "                \n",
    "        if found_types:\n",
    "            type_string = '_'.join(found_types)\n",
    "            temp_list = [type_string]*len(year_df)\n",
    "            graph_type.extend(temp_list)\n",
    "        \n",
    "\n",
    "        # appending year\n",
    "        years_temp_list = year_df['year'].tolist()\n",
    "        years.extend(years_temp_list)\n",
    "\n",
    "        #for item in years_temp_list:\n",
    "        #    years.append(years_temp_list)\n",
    "        # appending lek\n",
    "        leks_temp_list = year_df['lek_id'].tolist()\n",
    "        #leks.extend(leks_temp_list)\n",
    "        for lek in leks_temp_list:\n",
    "            leks.append(lek)\n",
    "        # appending easting\n",
    "        easting_temp_list = year_df['x_easting'].tolist()\n",
    "        x_easting.extend(easting_temp_list)\n",
    "        # appending northing\n",
    "        northing_temp_list = year_df['y_northing'].tolist()\n",
    "        y_northing.extend(northing_temp_list)\n",
    "        \n",
    "        #Create the graph from the file\n",
    "        graph = ig.Graph.Read_Adjacency(file_path)\n",
    "\n",
    "        # hub scores from be kleinberg's hub scores\n",
    "        hub_scores.extend(graph.hub_score())\n",
    "\n",
    "        # stepping stones from betweenness centrality\n",
    "        centrality_scores.extend(graph.betweenness())\n",
    "        \n",
    "        # cutpoints from articulation points\n",
    "        #articulation_status = []\n",
    "        #articulation_points = graph.articulation_points()\n",
    "        #for lek in leks_temp_list:\n",
    "        #    is_articulation = graph.vs.find(name = lek).index in articulation_points\n",
    "        #    articulation_status.append(is_articulation)\n",
    "        #articulation_point.extend(articulation_status)\n",
    "\n",
    "    \n",
    "    data = {'lek' : leks,\n",
    "            'x_easting' : x_easting,\n",
    "            'y_northing' : y_northing,\n",
    "            'graph_type' : graph_type,\n",
    "            'year' : years,\n",
    "            'hub_score' : hub_scores,\n",
    "            'centrality_score' : centrality_scores\n",
    "            #'articulation_point' : articulation_point,\n",
    "            }\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data\n",
    "df = pd.read_csv(\"E:/!!Research/!!!Data/graph_analysis/lek_data/lek_data_binary_rolling_activity.csv\")\n",
    "# dropping the column from the last time was exported\n",
    "df.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "# Finding only leks active per year\n",
    "df_active = df[df['active_last_5_years'] == 'TRUE']\n",
    "\n",
    "# creating df for each type of graph\n",
    "average_dispersal_df = node_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/average_dispersal', df_active)\n",
    "round_trip_df = node_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/round_trip', df_active)\n",
    "thresh48_df = node_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/threshold_4_8', df_active)\n",
    "max_disperse_df = node_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/threshold_max_disperse', df_active)\n",
    "coalescence_df = node_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/coalescence', df_active)\n",
    "\n",
    "# merging into final csv\n",
    "frames = [average_dispersal_df, round_trip_df, thresh48_df, max_disperse_df, coalescence_df]\n",
    "frame_concat = pd.concat(frames)\n",
    "frame_concat.to_csv(\"E:/!!Research/!!!Data/graph_analysis/igraph_metrics_data/lek_rolling_binary_node_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_metrics_df(given_directory, given_df):\n",
    "    # graph_type - already have a list of keywords to look for\n",
    "    types_list = ['average', 'dispersal', 'disperse', 'coalescence', 'round', 'trip', '48', 'max']\n",
    "    leks = [] # lek id\n",
    "    # lek coordinates\n",
    "    x_easting = []\n",
    "    y_northing = []\n",
    "    \n",
    "    graph_type = [] # graph type\n",
    "    years = [] # to easily find the year\n",
    "    hub_scores = []\n",
    "    centrality_scores = []\n",
    "    #articulation_point = []\n",
    "\n",
    "\n",
    "    adjacency_files = [f for f in os.listdir(f'{given_directory}') if f.endswith('.adjacency')]\n",
    "\n",
    "    for adj_file in adjacency_files:\n",
    "        file_path = f'{given_directory}/{adj_file}'\n",
    "        path_parts = adj_file.split('.')\n",
    "        parts = []\n",
    "        for piece in path_parts:\n",
    "            pieces = piece.split('_')\n",
    "            parts.append(pieces)\n",
    "\n",
    "        year = parts[0][0]\n",
    "        year_df = given_df[given_df['year'] == int(year)]\n",
    "\n",
    "        found_types = []\n",
    "        for part in parts[0]:\n",
    "            if part in types_list:\n",
    "                found_types.append(part)\n",
    "                \n",
    "        if found_types:\n",
    "            type_string = '_'.join(found_types)\n",
    "            temp_list = [type_string]*len(year_df)\n",
    "            graph_type.extend(temp_list)\n",
    "        \n",
    "\n",
    "        # appending year\n",
    "        years_temp_list = year_df['year'].tolist()\n",
    "\n",
    "        for item in years_temp_list:\n",
    "            years.append(years_temp_list)\n",
    "        # appending lek\n",
    "        leks_temp_list = year_df['lek_id'].tolist()\n",
    "        #leks.extend(leks_temp_list)\n",
    "        for lek in leks_temp_list:\n",
    "            leks.append(lek)\n",
    "        # appending easting\n",
    "        easting_temp_list = year_df['x_easting'].tolist()\n",
    "        x_easting.extend(easting_temp_list)\n",
    "        # appending northing\n",
    "        northing_temp_list = year_df['y_northing'].tolist()\n",
    "        y_northing.extend(northing_temp_list)\n",
    "        \n",
    "        #Create the graph from the file\n",
    "        graph = ig.Graph.Read_Adjacency(file_path)\n",
    "\n",
    "        # hub scores from be kleinberg's hub scores\n",
    "        hub_scores.extend(graph.hub_score())\n",
    "\n",
    "        # stepping stones from betweenness centrality\n",
    "        centrality_scores.extend(graph.betweenness())\n",
    "        \n",
    "        # cutpoints from articulation points\n",
    "        #articulation_status = []\n",
    "        #articulation_points = graph.articulation_points()\n",
    "        #for lek in leks_temp_list:\n",
    "        #    is_articulation = graph.vs.find(name = lek).index in articulation_points\n",
    "        #    articulation_status.append(is_articulation)\n",
    "        #articulation_point.extend(articulation_status)\n",
    "    \n",
    "    print(len(leks))\n",
    "    print(len(x_easting))\n",
    "    print(len(y_northing))\n",
    "    print(len(graph_type))\n",
    "    print(len(years))\n",
    "    print(len(hub_scores))\n",
    "    print(len(centrality_scores))\n",
    "    #print(len(articulation_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "[4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644]\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "# importing the data\n",
    "df = pd.read_csv(\"E:/!!Research/!!!Data/graph_analysis/lek_data/lek_data_binary_rolling_activity.csv\")\n",
    "# dropping the column from the last time was exported\n",
    "df.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "# Finding only leks active per year\n",
    "df_active = df[df['active_last_5_years'] == 'TRUE']\n",
    "\n",
    "average_dispersal_df = node_metrics_df('E:/!!Research/!!!Data/graph_analysis/igraph_graphs/test_folder', df_active)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per cluster metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
